= 第 7 章 有状态的流

有状态的流是事件驱动型微服务最为重要的组成部分，这是因为大部分应用程序需要为它们的处理需求或多或少地维护状态。2.4 节简单介绍过将事件流物化到本地状态的原则。本章将更深入地介绍事件驱动型微服务要如何构建、管理和使用状态。

[#_7_1_状态存储与从事件流中物化状态]
== 7.1 状态存储与从事件流中物化状态

先来了解两个定义。

物化状态（materialized state）::
来自源事件流的事件投影（不可变）。

状态存储（state store）::
存储服务业务状态的地方（可变）。

物化状态和状态存储都是有状态的微服务所需要并广泛使用的，但能够对二者加以区分十分重要。物化状态使你能够在微服务应用程序中使用公共的业务实体，而状态存储能让你存储业务状态和中间计算结果。每个微服务的设计都必须考虑要将服务数据存储在哪里。存储和访问状态有两种主要方式。

* 内部方式：数据与处理程序一起存储在同一个容器里，一般存储在内存中或磁盘上。
* 外部方式：数据存储在处理程序的容器之外，以某种外部存储服务的形式进行存储。这通常要通过网络请求来完成。

图 7-1 展示了内部状态存储和外部状态存储的例子。

.图 7-1：内部状态存储和外部状态存储
image::image101.png[]

选择内部状态存储还是外部状态存储主要取决于微服务的业务职责和技术需求。但是，在更深入地评估这两种方式之前，需要先了解变更日志所扮演的角色。

[#_7_2_记录状态到变更日志事件流]
== 7.2 记录状态到变更日志事件流

变更日志记录了对状态存储中的数据做的所有变更。它是“表–流”二元性中的“流”，状态表被转换成包含独立事件的流。作为在微服务之外维护的持久性状态副本，变更日志可用于重建状态，并且在事件处理进程中可以起到检查点的作用，如图 7-2 所示。

.图 7-2：启用了变更日志的状态存储
image::image102.png[]

image:image3.png[image,width=40,height=46]
变更日志优化了重建故障服务的任务，因为它们存储了之前的处理结果，这使得恢复处理程序避免了重新处理所有的输入事件。

变更日志流与其他流一样存储在事件代理中，如前所述，它们提供了重建状态存储的方法。变更事件流可以进行压缩，因为它们只需要用最新的键/值对来重建状态。

变更日志能够以高效的方式进行伸缩以及状态恢复，特别对内部状态存储来说更是如此。在这两种方式下，新创建的应用程序实例都需要从相关的变更日志分区中加载数据，如图 7-3 所示。

.图 7-3：正从变更日志中恢复状态存储
image::image103.png[]

变更日志可以作为内置功能提供，比如在 Kafka Streams 客户端中，或者由应用程序开发者来实现。基础的生产者 / 消费者客户端不会提供对变更日志或状态的支持。

[#_7_3_将状态物化至内部状态存储]
== 7.3 将状态物化至内部状态存储

内部状态存储与微服务业务逻辑共同存在于相同的容器或虚拟机环境中。特别是，内部状态存储的存在依附于微服务实例的存在，它们都运行在相同的底层硬件上。

每个微服务实例从分配给它的分区中物化事件，保持各个分区的数据在存储中的逻辑分离。这些逻辑分离的物化分区可以让微服务实例在消费者组再平衡之后简单地删除废除分区中的状态。通过保证物化状态只存在于拥有对应分区的实例上面，避免了资源泄露和多个事实来源的情况。通过消费来自事件流或变更日志的输入事件可以重建新的分区状态数据。

高性能的键/值存储（比如 RocksDB）通常被用于实现内部状态存储，并用本地固态驱动器（SSD）进行性能优化，以实现对超出所分配内存大小的数据集的性能操作。虽然键/值存储是最常用的内部状态存储，但也可以使用任何其他数据存储形式。关系或文档数据存储实现并非闻所未闻，但同样，它们需要被实例化并包含在每个单独的微服务实例中。

[#_7_3_1_物化全局状态]
=== 7.3.1 物化全局状态

全局状态存储是内部状态存储的一种特殊形式。与只物化分配给定的分区不同，全局状态存储能够物化给定事件流的所有分区的数据，以向每个微服务实例提供事件数据的完全副本。图 7-4 展示了全局物化状态和非全局物化状态的不同。

.图 7-4：全局物化状态与非全局物化状态
image::image104.png[]

当每个实例都需要一个完整的数据集时，全局状态存储非常有用，并且倾向于包含小的、常用的、很少变化的数据集。全局物化不能有效地作为事件驱动逻辑的驱动器，因为每个微服务实例都拥有数据的完整副本，因此会产生重复的输出和不确定的结果。所以，最好只是将全局物化用于通用数据集查找和维度表。

[#_7_3_2_使用内部状态的优点]
=== 7.3.2 使用内部状态的优点

pass:[1. 开发人员不用考虑可伸缩性要求]

使用本地磁盘上的内部状态存储的主要优点是，所有的可伸缩性要求完全转移到了事件代理和计算资源集群上面。这使得应用程序开发团队可以高度聚焦于编写应用逻辑，而同时依靠微服务能力团队来提供对所有事件驱动型微服务都通用的伸缩机制。这种方法保证了单一的可伸缩单元，每个应用程序都可以通过简单地增加和减少实例数量进行伸缩。

当考虑内部状态存储器时，了解应用程序的性能要求是很重要的。在现代云计算背景下，本地磁盘并不一定意味着是物理连接的磁盘，因为网络连接的磁盘可以模拟本地磁盘并为应用程序提供相同的逻辑支持。一个高吞吐量的有状态流微服务可以轻松地每秒消费成百上千个事件。必须谨慎考虑应用程序所要求的性能特性以确保能够满足延迟性方面的要求。

pass:[2. 基于磁盘的高性能方案]

在事件驱动型微服务中，维护主存内的所有状态通常是不可能的，特别是在保持低成本的情况下。对大多数现代微服务来说，物理连接的本地磁盘性能相当好。本地磁盘实现倾向于使用高随机访问模式，通常由 SSD 支持。例如，使用 RocksDB 从 SSD 中进行随机读的延迟大约是 65 毫秒，这意味着单个线程的顺序访问上限约为 154 000 个请求/秒。在内存中的话性能明显会更快，正常情况下每

秒可服务上百万的随机访问请求。本地磁盘和本地内存方法可带来非常高的吞吐量并显著降低数据访问瓶颈。

pass:[3. 使用网络连接磁盘的灵活性]

微服务也可以使用网络连接的磁盘代替本地磁盘，这会显著增加读/写延迟。由于通常必须一次处理一个事件以保持时间顺序和偏移顺序，因此单个处理线程将花费大量时间等待读/写响应，这导致每个处理程序的吞吐量明显较低。对不需要高性能处理的有状态服务来说，这通常是可以的，但是如果事件量很大，则可能会出现问题。

访问存储在网络连接的磁盘上的“本地”数据要比访问存储在系统内存或物理连接的磁盘上的本地数据有更高的延迟。虽然配置本地 SSD 的 RocksDB 预估有 154 000 个请求/秒的吞吐量，但是如果引入往返时间只有 1 毫秒的网络延迟，对于相同的访问模式，也会将吞吐量上限降至只有 939 个请求/秒。虽然可以使用并行访问来减小这个差距，但要记住事件的消费和处理必须按偏移量的顺序进行，因此在许多情况下并不能执行并行操作。

网络连接磁盘的一个主要优点是可以在数据卷中维护状态，并根据需要迁移到新的处理硬件。当处理节点恢复时，可以重新连接网络磁盘，并且在停止的位置继续处理，而不是从变更日志流重建。这大大减少了中断时间，因为状态不再像本地磁盘那样完全是短暂的，而且当你使用廉价的按需节点时，还增加了微服务跨计算资源迁移的灵活性。

[#_7_3_3_使用内部状态的缺点]
=== 7.3.3 使用内部状态的缺点

pass:[1. 仅限于使用运行时定义的磁盘]

内部状态存储仅限于使用在服务运行时定义并连接到节点的磁盘。变更所连接数据卷的大小和数量通常要暂停服务、调整卷，然后重启服务。此外，许多计算资源管理方案只允许增加数据卷的大小，因为减少卷的大小意味着需要删除数据。

pass:[2. 浪费磁盘空间]

具有周期性的数据模式，比如下午 3 点到凌晨 3 点对购物网站产生的流量，需要周期性的存储量。也就是说，这些模式可能需要为峰值流量准备最大容量的磁盘空间，而在其他情况下只需要少量磁盘空间。与使用按存储的每字节数据收费的外部服务相比，为整个时间段保留完整磁盘会浪费空间和金钱。

[#_7_3_4_内部状态的伸缩和恢复]
=== 7.3.4 内部状态的伸缩和恢复

从状态恢复的角度看，将处理扩展到多个实例和恢复故障实例是相同的过程。新实例或恢复的实例需要物化其拓扑定义的所有状态，然后才能开始处理新事件。最快的方法是为应用程序中物化的每个有状态存储重新加载变更日志主题。

pass:[1. 使用热副本]

虽然每个分区只有一个物化状态副本是最常见的情况，但是可以通过一些细致的状态管理来创建额外的副本，或者直接利用客户端框架的能力。Apache Kafka 可通过设置一个简单的配置将此功能内置到流框架内。这个设置提供了高可用的状态存储并使得微服务可以适应实例故障而不会有中断时间。

图 7-5 展示了内部状态存储复制因子为 2 的 3 个实例的部署。对每个有状态的分区都物化了两次，一个作为首领，一个作为副本。每个副本都必须管理自己的偏移量以确保跟首领副本的偏移量保持一致。实例 0 和实例 1 正在处理流 B 的事件，并将它们与协同分区的物化状态进行联结。实例1 和实例 2 也在分别维护流 A-P0 和流 A-P1 的热副本，但是实例 2 并不处理任何其他事件。

.图 7-5：拥有 3 个实例且每个物化输入分区有两个热副本的流 – 表联结
image::image105.png[]

当首领副本所在节点终止工作时，消费者组必须再平衡分区的分配。分区分配器会确定热副本的位置（它之前分配了所有分区，并且知道所有分区到实例的映射）并据此重新分配分区。在图 7-6 中，实例 1 已经终止工作，而剩下的微服务被迫再平衡它们的分区分配。具有热副本的实例会优先声明对分区的所有权并马上恢复处理。分区分配器已挑选实例 2 来恢复对流 B-P1 的处理。

.图 7-6：由于实例 1 终止而进行再平衡
image::image106.png[]

一旦恢复处理，就必须从变更日志中构建新的热副本以保证有最小副本数量。如图 7-7 所示，新的热副本被构建并添加到剩下的实例中。

.图 7-7：拥有两个实例且每个物化输入分区有两个热副本的常规操作
image::image107.png[]

image:image4.png[image,width=40,height=46]
热副本方法的一个主要折中点就是使用额外的磁盘来维持多副本，以换取在实例发生故障时中断时间的减少。

pass:[2. 从变更日志中恢复和扩展]

当一个新创建的微服务实例加入消费者组时，所有分配给它的有状态分区都可以简单通过消费其变更日志进行重新加载。在此期间实例不能处理新事件，因为如果这样做会产生不确定和错误的结果。

pass:[3. 从输入事件流中恢复和扩展]

如果没有维护变更日志，那么微服务实例可以从输入流中重建其状态存储。它必须从分配给它的事件流分区的开始位置重新消费所有输入事件。必须以严格递增的顺序消费并处理每个事件，更新其状态，并且产生后续的输出事件。

image::image5.png[image,width=47,height=44]

要考虑一下在完全重新处理期间所产生的事件的影响。下游消费者可能需要幂等地处理这些事件或者将它们作为重复事件清除。

比起从变更日志恢复的方法，这个重建状态过程可能要花费较长的时间。所以，最好只对那种重复输出不是问题、输入事件流维持时间短以及实体事件流很稀疏的拓扑采用此策略。

[#_7_4_将状态物化至外部状态存储]
== 7.4 将状态物化至外部状态存储

外部状态存储存在于微服务的容器或虚拟机外部，但通常位于同一个本地网络内部。虽然可以使用偏好的技术来实现外部数据存储，但是应该基于微服务问题域的需要来做出选择。一些通用的外部存储服务例子包括关系型数据库，文档型数据库，基于 Lucene 的地理空间搜索系统，以及分布式、高可用的键/值存储。

要记住，虽然一个微服务的外部状态存储可以使用公共的数据存储平台，但是数据集本身应该与所有其他微服务的实现保持逻辑上的隔离。对想要使用公共的物化数据集来服务多个业务需求的外部数据存储实现者来说，在微服务之间共享物化状态是一种常见的反模式。这会导致完全不相关的产品和特性之间的强耦合，应该避免。

image:image5.png[image,width=47,height=44]
不要与其他微服务共享直接的状态访问。相反，所有的微服务必须物化自己的状态副本。这就消除了微服务间直接的耦合并且隔离了无意的变更导致的问题，但代价就是要有额外的处理和数据存储资源。

[#_7_4_1_外部状态的优点]
=== 7.4.1 外部状态的优点

pass:[1. 完全数据局部性]

不像内部状态存储，外部状态存储可以为各个微服务实例提供对所有物化数据的访问，尽管每个实例仍然只负责物化分配给它的分区。当你在执行查找操作、依据外键进行关系查询以及在大量元素中执行地理空间查询时，有单一的物化数据集就不再需要分区局部性了。

image::image3.png[image,width=40,height=46]

使用具有强“数据一致性”保证的状态存储可以消除使用多个实例时的不一致结果。

pass:[2. 技术]

外部数据存储可以利用组织已经熟悉的技术，减少将微服务部署到生产环境要花费的时间和精力。如第 10 章所述，基础的消费者/生产者模式就特别适用于外部数据存储。如第 9 章所述，FaaS 的方案对外部数据存储来说也是很棒的选项。

[#_7_4_2_外部状态的缺点]
=== 7.4.2 外部状态的缺点

pass:[1. 管理多种技术]

外部状态存储是独立于微服务业务逻辑方案而进行管理和伸缩的。外部数据存储的一个风险是，微服务所有者现在要负责确保它得到适当的维护和伸缩。每个团队必须实施适当的资源分配、扩展策略和系统监控措施，以确保他们的数据服务能力匹配微服务器的负载。由组织的效能团队或第三方云平台提供的数据管理服务有助于分担部分责任。

image:image4.png[image,width=40,height=46]
每个微服务团队必须完全管理外部状态存储。不要将外部状态存储的管理责任委托给存储团队，因为这会引入跨团队的技术依赖。列出一个可接受的外部数据服务列表，并提供如何正确管理和伸缩这些服务的指南。这可以避免让每个团队都不得不独立地探寻自己的管理解决方案。

pass:[2. 由于网络延迟造成的性能损耗]

与访问位于内存中或磁盘上的数据相比，访问外部状态存储的数据有更高的延迟。前面介绍过，使用网络连接的磁盘带来了轻微的网络延迟，并且显著降低了吞吐量和性能。

虽然缓存和并行化可以减少网络延迟的影响，但是通常会加入复杂度并增加额外的内存和 CPU 开销。并不是所有的微服务模式都能支持缓存和并行化方案，有许多模式要求处理线程进行阻塞并等待来自外部数据存储的响应。

pass:[3. 外部状态存储服务的财务成本]

外部数据存储的财务成本要比相同规模的内部数据存储高。托管的外部状态存储解决方案通常是按事务数、数据负载大小和数据的保留期收费。它们可能还需要过度配置以应对突发和不一致的负载。具有灵活的性能特点的按需定价模型可能有助于降低成本，但必须确保它们仍能满足性能要求。

pass:[4. 完全数据局部性]

完全数据局部性虽然被列为一项优点，但是它也带来了一些挑战。外部状态存储中的可用数据来源于多个处理程序和多个分区，每个处理程序和分区都以自己的速率进行处理。这就很难推断和调试任何特定处理实例对共享状态的贡献。

同时也必须小心避免竞争条件和不确定性行为，因为每个微服务实例都是在自己独立的流时间上运行的。单个微服务实例对流时间的保证不会扩展到所有实例。

例如，一个实例可能试图用外键联结事件，而该外键尚未由另一个单独的实例填充。当稍后再次处理相同的数据时则可以执行该联结。因为每个实例的流处理完全独立于其他实例，所以通过此方法获得的任何结果都可能是不确定和不可复制的。

[#_7_4_3_外部状态存储的伸缩和恢复]
=== 7.4.3 外部状态存储的伸缩和恢复

使用外部状态存储的微服务，其扩容和恢复只需要添加新的实例，只要该实例有访问状态存储所需的凭据。相反，底层状态存储的伸缩和恢复则完全依赖于所选择的技术，这会复杂得多。

重申前面的一个观点，拥有一个可接受的外部数据服务列表以及如何正确管理、伸缩、备份和恢复这些服务的指南，对于为开发人员提供可持续的前进道路至关重要。不幸的是，状态存储技术的数量多得令人望而却步，而且实际上不可能在本书中进行讨论。接下来，我将简单地将构建状态的策略概括为 3 种主要技术：从源流重建、使用变更日志和创建快照。

pass:[1. 从源流重建]

从源流的最开始时间消费事件可以创建状态存储的全新副本。对于所有输入流，消费者组的输入偏移量被重置到最开始时间。在所有方法中，此方法的中断时间最长，但易于复制，并且只依赖于事件代理的持久性存储来维持源数据。请记住，此方法是真正完全的应用程序重置，并且还会根据微服务的业务逻辑再次产生所有输出事件。

pass:[2. 使用变更日志]

尽管没有规则阻止，但是外部状态存储通常不会依赖于代理存储的变更日志来记录和恢复状态。与内部状态存储很像，外部状态存储也可以从变更日志中进行填充。就像从源流中执行重建一样，必须创建一份全新的状态存储副本。如果从变更日志中重新构建，那么微服务消费者实例必须确保在恢复处理之前重新构建出存储在变更日志中的全部状态。

image:image5.png[image,width=47,height=44]
由于网络延迟开销，从源事件流或变更日志中重建外部状态存储会非常耗时。请确保在此场景中仍然能够满足微服务的 SLA。

pass:[3. 创建快照]

对于外部状态存储，更常见的是由它们提供自身的备份和恢复处理机制，并且许多托管的状态存储服务会提供简单的“一键式”解决方案。应该根据给定的状态存储实现来遵循捕获和恢复状态的最佳实践。

如果存储状态是幂等的，则无须确保偏移量与物化状态精确一致。在这种情况下，将消费者偏移量设置为保存快照前几分钟的值，可以确保不会丢失任何数据。这也确保了事件的处理具有“至少一次”的保证。

如果存储的状态不是幂等的，并且任何重复的事件都是不可接受的，那么应该将消费者的分区偏移量与数据存储区中的数据一起存储。这确保了消费者偏移量和相关的状态是一致的。当从快照恢复状态时，消费者可以将其消费者组偏移量设置为自创建快照的确切时间起在快照中找到的偏移量。7.6.3节将对此进行更详细的介绍。

[#_7_5_重建与迁移状态存储]
== 7.5 重建与迁移状态存储

对现有状态存储数据结构的更改通常需要伴随新的业务需求。微服务可能需要向现有事件添加新信息，与另一个物化表执行一些额外的联结步骤，或者存储新派生的业务数据。在这种情况下，需要通过重建或迁移来更新现有的状态存储以反映数据。

[#_7_5_1_重建]
=== 7.5.1 重建

重建微服务的状态存储通常是更新应用程序内部状态的最常见方法。微服务首先要停机，然后将消费者输入流偏移量重置为开始位置。重建必须删除所有中间状态，比如存储在变更日志中或位于外部状态存储中的状态。最后，启动新版本的微服务，从输入事件流中读取事件并重建状态。该方法确保了完全按新业务逻辑指定的方式构建状态。同时也会创建所有新的输出事件并向下游传播给订阅消费者。这些事件不被视为重复事件，因为业务逻辑和输出格式已经更改，而且这些更改必须传播到下游。

重建状态要求所有必需的输入事件流事件仍然存在，特别是任何需要物化状态和聚合的事件。如果应用程序严重依赖于一组输入数据，则必须确保这些源数据在微服务实现的数据存储之外随时可用。

image:image3.png[image,width=40,height=46]
重建需要时间，在微服务的 SLA 中考虑到这一点很重要。演练重建的一个主要好处是，通过运行当微服务发生故障且所有状态都丢失时所需的恢复过程，可以帮助你测试灾难恢复准备情况。

最后，有些业务需求要求从一开始就重新处理数据，比如那些提取只出现在输入事件中的字段的数据。除了重放输入事件之外，无法从任何其他方式获取此数据，此时重建状态存储是唯一可行的选择。

[#_7_5_2_迁移]
=== 7.5.2 迁移

与变更所带来的影响相比，大型状态存储需要很长时间来重建，或者会导致高昂的数据传输成本。假设发生了一个业务需求变更，其中一个附加（但可选）的字段将被添加到微服务的输出事件流中。这个变更可能需要往微服务的状态存储中添加一个列或字段。但是，这个业务可能不需要重新处理旧数据，只需将逻辑应用于未来的新输入事件。对于由关系型数据库支持的状态存储，你只需要更新与相关表定义有关系的业务逻辑。可以执行一次简单的新列插入，使其允许接受为空的默认值，并且在一系列快速测试之后，重新部署应用程序。

当业务需求和变更的数据更复杂时，迁移的风险就更大。复杂的迁移容易出错，与完全重建数据存储的结果相比，更可能产生不正确的结果。数据库迁移逻辑不是业务逻辑的一部分，因此可能会导致不一致的情况，这在应用程序的完全重建过程中是不会出现的。如果没有在测试期间捕获到这种迁移错误，往后就很难检测到，并可能导致数据不一致。当使用基于迁移的方法时，一定要执行严格的测试，并使用具有代表性的测试数据集将该方法与基于重建的方法进行比较。

[#_7_6_事务与有效一次处理]
== 7.6 事务与有效一次处理

有效一次处理确保了对单一事实来源的任何更新都始终得到应用，无论生产者、消费者或事件代理发生任何故障。虽然不是很准确，但有效一次处理有时也被描述为精确一次处理。微服务可能会多次处理相同的数据，比如由于消费者发生故障且随后恢复，但没有提交其偏移量和递增流时间而重复消费事件。每处理一次事件都会执行一次处理逻辑，包括代码可能造成的任何副作用，比如发布数据到外部端点或者与第三方服务通信。也就是说，对大多数事件代理和大多数用例来说，术语精确一次和有效一次可以互换使用。

幂等写入是事件代理实现（比如 Apache Kafka 和 Apache Pulsar）中普遍支持的功能之一。它们允许一个事件只被写入事件流一次。如果生产者或事件代理在写入时发生故障，幂等写入功能会确保在重试时不创建重复的事件。

事件代理也可以支持事务特性。目前，只有 Apache Kafka 提供了完整的事务支持，尽管 Apache Pulsar正在朝着自己的实现努力。与关系型数据库能够在单个事务中支持多次更新非常类似，事件代理实现也可以支持往多个不同的事件流中原子写多个事件。这使得生产者可以在单个原子事务中向多个事件流发布其事件。缺乏事务支持的事件代理竞品要求客户端保证自己的有效一次处理。下一节的内容会涵盖这些选项并会评估你可以如何将它们用于自己的微服务。

image:image3.png[image,width=40,height=46]
事务是非常强大的功能，它使 Apache Kafka 在与其竞品的竞争中占据显著优势。特别是，它们可以很好地满足新业务的需求，否则将需要进行复杂的重构以确保原子生产。

[#_7_6_1_示例库存计算服务]
=== 7.6.1 示例：库存计算服务

库存计算服务负责在任何给定物品的库存不足时发出通知事件。微服务必须根据一系列随时间变化的加减法，将每个产品的现有可用库存拼凑在一起。向客户出售物品、由于损坏而丢弃物品以及由于偷盗而丢失物品等，都是会减少库存的事件，而接收货物和接受客户退货会增加库存。在这个例子中，为了简单起见，这些事件会显示在同一个事件流中，如图 7-8 所示。

.图 7-8：一个简单的库存计算服务
image::image109.png[]

这个库存计算服务非常简单。它根据事件流的变动计算当前正在运行的库存的总量，并将其存储在数据存储中。业务逻辑用阈值进行过滤并决定是否要向库存管理部门发送低库存或超卖库存的通知。必须确保每个输入事件有效一次地应用到聚合状态，因为多次应用是不正确的，绝对不能如此。这就是有效一次处理发挥作用的地方。

[#_7_6_2_使用客户端代理事务的有效一次处理]
=== 7.6.2 使用“客户端–代理”事务的有效一次处理

任何支持事务的事件代理都能帮助达成有效一次处理。使用这种方法，任何输出事件、由变更日志支持的内部状态的更新以及消费者偏移量的递增都被包裹在单个原子事务中。只有当这 3 个更新都存储在代理中各自的特定事件流中时，才有可能实现这一点。如图 7-9 所示，偏移量更新、变更日志更新以及输出事件提交是在单一事务内原子地执行的。

.图 7-9：“客户端–代理”事务——提交偏移量和变更日志
image::image110.png[]

生产者客户端和事件代理之间的原子事务会发布所有事件到它们对应的事件流中。如图 7-10 所示，如果出现生产者的持久性故障，那么代理会确保不提交事务中的任何事件。事件流消费者通常不会处理未提交事务的事件。消费者必须遵循偏移量顺序，因此它会阻塞，等待事务完成，然后继续处理事件。如果发生暂时性故障，那么生产者可以简单地重试提交事务，因为这是一个幂等操作。

.图 7-10：“客户端–代理”事务提交失败
image::image111.png[]

如图 7-11 所示，如果生产者在事务执行期间遭遇致命性异常，通过从变更日志恢复，可以简单地重建其替代实例。事件流的消费者组偏移量也可以被重置到偏移量事件流中最后已知的处理位置。

.图 7-11：用变更日志和之前的偏移量从代理恢复状态
image::image112.png[]

生产者一旦恢复运行就可以开始处理新的事务，并且事件代理会认为所有之前未完成的事务都是失败的并将它们取消。根据代理实现的不同，事务机制在某种程度上会有所不同，因此请确保了解清楚你所使用的事务机制。

[#_7_6_3_没有客户端代理事务的有效一次处理]
=== 7.6.3 没有“客户端–代理”事务的有效一次处理

对不支持“客户端–代理”事务的实现来说，要做到事件的有效一次处理也是可能的，尽管这需要更多的工作和对重复事件的仔细考虑。首先，如果上游服务无法提供有效一次事件的生产保证，那么它们就可能产生重复的记录。需要识别并过滤上游进程创建的重复事件。其次，需要在一个本地事务中对状态和偏移量管理进行更新，以确保对系统状态只应用一次事件处理逻辑。通过遵循此策略，客户端可以确保其处理程序产生的内部状态与输入事件流的逻辑描述保持一致。下面来更详细地看看这些步骤。

image:image5.png[image,width=47,height=44]
最好使用支持幂等写入的事件代理和客户端，而不是事后解决去重问题。前者可以很好地扩展到所有的消费者应用程序，后者则成本高昂且难以扩展。

pass:[1. 产生重复事件]

如果生产者已成功地将事件写入事件流，但是没能接收到写入确认且进行重试，或者在更新其消费者偏移量之前崩溃，则会产生重复事件。这两种场景有一些不同。

生产者没能接收到代理的确认并重试::

在这种场景中，生产者仍然在内存中有要生产的事件的副本。这些事件，如果被再次发布，则可能有相同的时间戳（如果它们使用的是事件创建时间）和相同的事件数据，但是会被分配新的偏移量。

生产者在写入事件之后、更新其消费者偏移量之前立即崩溃::

在这种情况下，生产者已经成功写入其事件，但是还没有更新其消费者偏移量。这意味着当生产者重试的时候，它将重复之前已经做过的工作，创建逻辑上完全相同的事件副本，但具有新的时间戳。如果处理是确定性的，那么事件会有相同的数据。此时仍然会分配新的偏移量。

image:image4.png[image,width=40,height=46]
许多事件代理支持幂等生产，这能减轻如上面两个场景描述的崩溃和重试带来的故障。它无法减少由于错误的业务逻辑而带来的重复问题。

pass:[2. 识别重复事件]

如果无法提供事件的幂等生产，并且在事件流中存在重复事件（具有唯一的偏移量和唯一的时间戳），那就需要你来消除它们的影响。首先，确定重复事件是否真的会导致任何问题。在许多情况下，重复事件的影响很小，甚至可以忽略不计。对于那些确实会因重复事件而导致问题的场景，需要指出如何识别它们。一种方式就是让生产者为每个事件生成一个唯一 ID，这样所有重复事件都会生成相同的唯一哈希函数。

此哈希函数通常基于内部事件数据的属性创建，包括键、值和事件创建时间。这种方法往往适用于具有大数据域的事件，对于逻辑上相互等效的事件则效果不佳。以下是一些可以生成唯一 ID 的场景：

* 银行账户转账，详细说明来源、目的地、金额、日期和时间；
* 电子商务订单，详细说明每个产品、购买者、日期、时间、总金额和付款提供商；
* 用于装运目的的借记库存，每个事件都有一个关联的 orderId（使用现有的唯一数据 ID）。

这些例子的一个共同点是，每个 ID 都由具有非常高基数（唯一性）的元素组成。这极大地降低了ID 之间重复的机会。用于消除重复的 ID，既可以与事件一起生成，也可以由消费者在消费时生成，前者更适合分发给所有消费者。

image:image5.png[image,width=47,height=44]
防范没有键的重复事件相当具有挑战性，因为没有分区局部性保证。只要有可能，就要产生键控事件、遵从分区局部性并使用幂等写入。

pass:[3. 防范重复事件]

任何有效的一次性消费者都必须识别并丢弃重复事件、执行幂等操作，或者从有幂等生产者的事件流中消费事件。不是所有的业务案例都能进行幂等操作，如果没有幂等生产，则必须找到一种防止业务逻辑受重复事件影响的方法。这可能是一项代价高昂的工作，因为它要求每个消费者维护一个以前处理过的去重 ID 的状态存储。随着事件量增加和应用程序必须覆盖的偏移量或时间范围的增大，该存储会变得非常大。

完美的去重要求每个消费者无限期地维护对已处理的各个去重 ID 的查找，但如果想防范的范围过大，那么时间和空间成本可能会变得非常高昂。实际上，去重通常只会对特定的滚动时间窗口或偏移窗口执行，以尽最大努力达成目的。

image:image3.png[image,width=40,height=46]
使用生存时间（TTL）、最大缓存大小和定期删除来保持较小的去重存储规模。根据应用程序对重复事件的敏感度和重复事件所产生的影响，这个设置需要的值会有所不同。

应该仅在单个事件流分区内尝试去重，因为分区之间的去重代价将非常高昂。与无键事件相比，键控事件有一个额外的好处，因为相同键的事件将被一致地分布到同一个分区。

图 7-12 展示了一个正在运行的去重存储。从这个图中可以看到事件在传递给真正的业务逻辑之前所经过的工作流。在本例中，TTL 被设置为 8000 秒，但实际上需要根据业务需求来设置该值。

.图 7-12：使用持久化状态进行去重
image::image116.png[]

image:image3.png[image,width=40,height=46]
在去重存储中使用最大缓存大小来限制维护的事件数，特别是在再处理期间。

注意，你要负责维护去重表的持久备份，就像维护任何其他物化表一样。如果发生故障，则必须在恢复处理新事件之前重建该表。

pass:[4. 保持一致状态]

微服务可以利用其状态存储的事务能力而不是事件代理能力来执行有效一次处理。这就要求将消费者组的偏移量管理从事件代理转移到数据服务中，让一个有状态的事务原子地更新状态和输入偏移量。对状态所做的任何更改都与对消费者偏移量所做的更改完全一致，这保持了服务内部的一致性。

如果服务失败，比如向数据服务提交数据时出现超时，则微服务可以简单地放弃事务并回退到上次已知的正确状态。此时服务会暂停所有消费，直到数据服务恢复响应，然后从上次已知的正常处理的偏移量开始恢复消费。通过保持偏移量的正式记录与数据服务中的数据同步，可以得到状态一致视图，服务可以从中恢复。这个过程如图 7-13、图 7-14 和图 7-15 所示。

.图 7-13：事件的正常事务处理
image::image117.png[]

.图 7-14：事务处理失败
image::image118.png[]

.图 7-15：状态恢复过程中的偏移量恢复
image::image119.png[]

注意，此方法让处理程序可以有效一次处理，而不是有效一次生产事件。该服务生成的所有事件都只是达到了至少一次生产的要求，因为非事务性的“客户端–代理”事件生产会遇到重复创建的问题。

如果必须在状态存储更新的同时以事务方式生产事件，请参阅第 4 章。使用变更数据表可以为状态存储提供最终一致且有效的一次更新，并为输出事件流提供至少一次生产。

[#_7_7_小结]
== 7.7 小结

本章介绍了内部状态存储和外部状态存储，它们如何工作，它们的优点和缺点，以及何时使用它们。数据局部性在系统的延迟和吞吐量方面起着很大的作用，使得可以在负载很重时进行扩容。内部状态存储可以提供高性能处理，外部状态存储在支持微服务的业务需求方面可以提供许多灵活选项。

变更日志在备份和恢复微服务的状态存储方面扮演着重要角色，尽管该角色还可以由支持事务的关系型数据库以及定期保存的快照来扮演。支持事务的事件代理可以实现非常强大的有效一次处理，从而减轻消费者防止重复的职责，而去重工作可以使没有此类支持的系统实现有效一次处理。
