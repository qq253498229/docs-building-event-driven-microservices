= 第 14 章 支持性工具

支持性工具使你能够高效地管理大规模的事件驱动型微服务。虽然这些工具中的大多数可以由管理员执行的命令行界面提供，但最好拥有一系列自助服务工具。这些工具提供了对于确保可伸缩和弹性的业务结构至关重要的 DevOps 功能。本章所涉及的工具并不是唯一可用的工具，但它们是我和其他人在实践中发现的有用工具。你的组织需要根据自己的实际情况来决定采用哪一种。

不幸的是，管理事件驱动型微服务缺乏免费的开源工具。在适当的情况下，我会列出可用的具体工具实现，但它们中的大部分是为我工作过的企业私下编写的。你可能需要编写自己的特定工具，但我鼓励你，如果可以的话，使用开源工具，并尽可能为其做出贡献。

[#_14_1_微服务团队分配系统]
== 14.1 微服务–团队分配系统

当一个公司拥有少量系统时，很容易使用部落知识或非正式方法来跟踪谁拥有哪些系统。在微服务领域，明确跟踪微服务实现和事件流的所有权是很重要的。通过遵循单一写原则（参见 2.6 节），可以将事件流所有权归于拥有写入权限的微服务。

可以内部开发一个简单的微服务来跟踪并管理人员、团队和微服务之间的所有依赖关系。这个系统是本章中许多其他工具的基础，所以我强烈建议你去寻找或开发它。以这种方式分配微服务所有权有助于确保将细粒度的 DevOps 权限正确分配给需要它们的团队。

[#_14_2_事件流的创建和修改]
== 14.2 事件流的创建和修改

团队需要有创建事件流和修改它们的能力。微服务应该有权自动创建自己的内部事件流，并完全控制像分区数、保留策略和复制因子这样的重要属性。

例如，包含极其重要和敏感数据的流可能具有无限保留策略和高复制因子。另外，包含大量单独的非重要更新事件的流可能具有高分区数、低复制因子和短保留策略。在创建事件流时，通常将其所有权分配给特定的微服务甚至外部系统。下一节会讨论这个问题。

[#_14_3_事件流元数据标记]
== 14.3 事件流元数据标记

一种分配所有权的有用技术是用元数据标记流。只有拥有流生产权限的团队才能添加、修改或删除元数据标记。有用的元数据示例包括但不限于以下这些。

流所有者（服务）::
拥有流的服务。在传递更改请求或审核哪些流属于哪些服务时，经常使用此元数据。它使组织中任何微服务或事件流的所有权和业务沟通结构更加清晰。

个人可识别信息（personally identifiable information，PII）::
这类信息需要更严格的安全保障措施，因为通过它可以直接或间接地识别用户。此元数据的一个基本用例是限制对任何标记为 PII 的事件流的访问，除非拥有数据的团队明确批准这样做。

财务信息::
任何与金钱、账单或其他重要的创收活动有关的信息。这类信息与 PII 相似，但不完全相同。

命名空间::
与业务的嵌套界限上下文结构对齐的描述符。分配了命名空间的流可以对命名空间外的服务隐藏，但对命名空间内的服务可用。这有助于通过隐藏不可访问的事件流来减少用户浏览可用事件流时的数据过载。

弃用::
表示流已过期或因某种原因被取代的一种方法。将事件流标记为“弃用”，这样旧系统便仍然可以使用它，但是新的微服务无法请求订阅。当必须对现有事件流的数据格式进行中断更改时，通常使用此标记。新的事件可以放入新的流中，旧的流则会一直保持，直到依赖的微服务被迁移走。最后，当弃用的流不再有注册的消费者时，通知弃用的事件流所有者，此时可以安全地删除它。

自定义标记::
其他任何适合你的业务的元数据都可以而且应该使用元数据标记进行跟踪。请思考哪些标记对你的组织很重要并确保它们可用。

[#_14_4_限额]
== 14.4 限额

限额通常由事件代理在全局层面进行设置。例如，可以将事件代理设置为只允许将 20% 的 CPU 处理时间用于为单个生产者或消费者组提供服务。限额可以防止突发的拒绝服务（由于突然活跃的生产者或高并发的消费者组从大数据量的事件流开始位置消费）。一般来说，至少要确保整个集群不会被一个服务的 I/O请求所占满。可以简单地限制消费者或生产者能够使用的资源数量，从而控制住资源。

你可能需要在更细粒度的级别上设置限额，以防易发生波动的系统受到限制，同时确保稳定的消费者的处理能力和网络 I/O 量达到最低。你可能需要为事件代理集群外的数据生产者设置不同的限额或取消其限额。假设一个生产者基于第三方输入流或外部同步请求发布事件，如果其生产事件的速率被限制在传入消息的速率以下，则可能会丢弃数据或崩溃。

[#_14_5_schema注册表]
== 14.5 schema注册表

显式 schema 为事件建模提供了强大的框架。数据的精确定义，包括名称、类型、默认值和文档，为事件的生产者和消费者提供了清晰的信息。schema 注册表是一种服务，它允许生产者注册他们用来编写事件的 schema。这提供了几个明显的好处：

* 事件 schema 不需要与事件一起传输。可以使用简单的占位符 ID，显著减少带宽的使用；
* schema 注册表为获取事件的 schema 提供了唯一参考；
* schema 支持数据发现，特别是全文本搜索。

schema 注册表的工作流如图 14-1 所示。

.图 14-1：生产和消费事件的 schema 注册表工作流
image::image447.png[]

在生产数据前的序列化事件时，生产者会向 schema 注册表注册 schema 以获取 schema 的 ID（步骤1)）。然后，它将 ID 附加到序列化事件中（步骤 2)），并将信息缓存在生产者缓存中（步骤 3)），以避免再次在注册表中查询该 schema。请记住，生产者必须为每个事件执行此过程，因此消除对已知事件格式的外部查询是必不可少的。

消费者接收事件并从其缓存或 schema 注册表中获取特定 ID 的 schema（步骤 4)）。然后，它用schema 替换 ID（步骤 5)），并将事件反序列化为已知格式。如果 schema 是新的，则缓存该模式（步骤 6)），现在消费者的业务逻辑可以使用反序列化的事件了。在此阶段，事件还可以应用 schema 演化。

Confluent 为 Apache Kafka 提供了一个出色的 schema 注册表实现。它支持 Apache Avro、Protobuf和 JSON 格式，并可免费用于生产环境。

image:image4.png[image,width=40,height=46]
将 schema 注册到专用的事件流可以使 schema 注册表实现免于提供持久的存储。这是Confluent 所选择的 schema 注册表实现方式。

[#_14_6_schema创建和修改通知]
== 14.6 schema创建和修改通知

事件流 schema 在标准化通信方面是很重要的。有一个问题会出现，特别是在有大量事件流的时候，那就是如果通知其他团队他们所依赖的 schema 发生了变化（或即将发生变化），就会出现问题。这就是schema 创建和修改通知要做的事情。

通知系统的目的只是在消费者的输入 schema 发生变化时提醒消费者。访问控制列表（ACL，将在本章后面讨论）是确定哪个微服务会消费哪个事件流以及依赖于哪些 schema 的一种很好的方法。

可以从 schema 流（如果你使用的是 Confluent 的 schema 注册表）中消费 schema 更新事件，并交叉引用到它们关联的事件流。从这里开始，ACL 提供了关于哪些微服务在消费哪些事件流的信息，然后通过微服务–团队分配系统通知相应负责这些服务的团队。

通知系统有许多好处。在一个完美的世界中，每一个消费者都能够全面地审查上游对 schema 的每一次变更，通知系统则提供了一个安全网，用于提前识别出有害的或破坏性的变更，以免发生真正的危机。最后，消费者可能希望了解公司中所有公开可用的 schema 变更，以便在新的事件流上线时更深入地了解数据。

[#_14_7_偏移量管理]
== 14.7 偏移量管理

事件驱动型微服务要求在处理数据之前管理偏移量。在正常操作中，微服务将在处理消息时前移其消费者偏移量。但是，在某些情况下，必须手动调整偏移量。

应用程序重置：重置偏移量::
变更微服务的逻辑可能需要你从之前的时间点重新处理事件。通常重新处理会从流的起始点进行，但你选择的点可以根据服务的需求有所不同。

应用程序重置：前移偏移量::
也许你的微服务不需要旧数据，只消费最新的数据。你可以重置应用程序偏移量到最近的点而不是最早的那个点。

应用程序恢复：指定偏移量::
你可能想把偏移量重置到特定的时间点。这通常在多集群故障转移中起作用，在这个过程中，你想要确保没有丢失任何消息，但不希望从最开始消费事件。一种策略是将偏移量重置为崩溃前 N 分钟的时间，以确保不会丢失任何复制的消息。

对于生产级 DevOps，团队必须是微服务的拥有者才能修改其偏移量，这是微服务–团队分配系统提供的功能。

[#_14_8_事件流的权限和访问控制列表]
== 14.8 事件流的权限和访问控制列表

对数据的访问控制不仅从业务安全的角度来看是重要的，而且也是实施单一写原则的一种手段。权限和访问控制列表（ACL）确保了界限上下文可以强制限定它们的边界。对给定事件流的访问权限只能由拥有生产微服务的团队授予，你可以使用微服务–团队分配系统来实施这一限制。权限通常分为以下几类（当然，这取决于事件代理的实现）：读、写、创建、删除、修改和描述。

image:image5.png[image,width=47,height=44]
ACL 依赖于每个消费者和生产者的个人标识。确保尽快为你的事件代理和服务启用并强制实施标识，最好从一开始就执行。事后添加标识是非常痛苦的，因为它需要更新和检查连接到事件代理的每个服务。

ACL 强化了界限上下文。例如，一个微服务应该只拥有对其内部事件流和变更事件流的创建和读写权限。任何时候，微服务都不应该耦合到另一个微服务的内部事件流上。此外，根据单一写原则，这个微服务应该是唯一分配了其输出流写权限的服务。输出流可以公开，使得任何其他系统都可以消费该数据，或者由于其包含敏感的金融数据或 PII 数据或是嵌套的界限上下文的一部分而具有受限的访问。

典型的微服务将按照表 14-1 所示的格式单独分配一组权限。

表 14-1：微服务的典型事件流权限

|===
|组成 |微服务权限
|输入事件流
|读
|输出事件流
|创建、写（如果是内部使用，也可能有读）
|内部事件流和变更日志事件流
|创建、写、读
|===

一种有效的做法是，授予每个团队分配特定微服务的消费访问权限的权力，从而将访问控制执行的责任转移给他们。或者，根据业务需求和元数据标记，可以将此过程集中化，以便团队在请求访问敏感信息时进行安全审查。权限的授予和撤销可以作为自己的事件流保存，为审计提供持久不变的数据访问记录。

发现孤立的流和微服务

在正常的业务增长过程中，会创建新的微服务和流，并删除弃用的微服务和流。交叉引用现有流和微服务的访问权限列表有助于检测孤立项。如果流没有消费者，则可以将其标记为可删除。如果那个事件流的生产者微服务不向其他有消费者的事件流中产生数据，则它也可以被移除。通过这种方式，你可以利用权限列表来保持事件流和业务拓扑的健康和更新。

[#_14_9_状态管理和应用程序重置]
== 14.9 状态管理和应用程序重置

在变更有状态的应用程序时重置应用程序的内部状态是很普遍的。对存储在内部事件流和变更日志事件流中的数据结构的任何变更，以及对拓扑工作流的任何变更，都需要根据新应用程序的要求删除和重新创建流。

第 7 章讨论的一些有状态的微服务模式会使用在处理节点外部的状态存储。根据你所在公司的微服务平台部门所支持的功能，当微服务所有者提出请求时，可以重置这些外部状态存储。如果一个微服务正在使用外部状态存储，比如亚马逊的 DynamoDB 或谷歌的 Bigtable，那么重置应用程序时最好清除关联的状态。这减少了操作开销，并会确保自动删除任何过时或错误的数据。“官方支持的功能”领域之外的任何外部有状态服务都可能需要人工重置。

需要注意的是，虽然这个工具应该是自助服务的，但是一个团队决不能删除另一个团队拥有的事件流和状态。同样，我建议使用本章讨论的微服务–团队分配系统，以确保应用程序只能由其所有者或管理员重置。

总之，此工具需要：

* 删除微服务的内部流和变更日志流；
* 删除任何外部状态存储的物化（如果适用的话）；
* 将消费者组偏移量重置到每个输入流的开始位置。

[#_14_10_消费者偏移量滞后度监控]
== 14.10 消费者偏移量滞后度监控

消费者滞后度是事件驱动型微服务需要扩容的最佳指标之一。你可以使用工具来监控这一指标，该工具定期计算相关的消费者组的滞后度。尽管不同代理实现的机制可能有所不同，但是滞后的定义是相同的：最近的事件与给定的微服务消费者组上一次处理的事件之间的数量之差。滞后度的基本度量（比如阈值度量）是相当简单和容易实现的。如果一个消费者的偏移量滞后度大于 N 个事件达 M 分钟，则会触发消费者处理器数量加倍并再平衡工作负载。如果滞后已解决，并且当前运行的处理器数高于所需的最小值，请按比例减少处理器数量。

一些监控系统（比如 Apache Kafka 的 Burrow），在计算滞后状态时考虑了偏移量滞后度的历史情况。这种方法在有大量事件进入流的情况下非常有用，例如，在下一个事件到达之前的一瞬间，滞后量仅为0。因为滞后度测量在本质上往往是周期性的，所以使用常规测量时，系统可能总是显得滞后。因此，利用与历史数值的偏差可以成为一种有效的机制，用以确定一个系统是落后还是正在赶上事件进度。

请记住，虽然微服务可以根据需要自由地扩缩容，但通常会用某种形式的迟滞（比如使用容忍阈值）手段来防止系统无限地扩缩容。这种迟滞反馈需要成为信号评估逻辑的一部分，并且通常可以被现代云平台 [比如 AWS CloudWatch 和 Google Cloud Operations（以前叫 Stackdriver）] 所适配。

[#_14_11_流水线型的微服务创建流程]
== 14.11 流水线型的微服务创建流程

为新的业务需求创建代码仓库在微服务环境中是一个典型任务。将此任务自动化为一个流水线过程将确保所有内容都可以组合在一起，并集成到效能团队提供的通用工具中。

以下是一个典型的微服务创建流程。

1. 创建一个仓库。
2. 使用持续集成管道创建任何必要的集成（参见 16.2.1 节）。
3. 配置任何 Webhook 或其他依赖项。
4. 使用微服务–团队分配系统给团队分配所有权。
5. 注册输入流的访问权限。
6. 创建所有输出流并应用所有者权限。
7. 提供应用模板或代码生成器的选项以创建微服务的框架。

团队将重复多次执行这个流程，所以将其流水化会极大地节省时间和工作量。新的自动化工作流包括一个用于最新模板和代码生成器的注入点，确保新项目包含最新支持的代码和工具，而不是简单地复制旧项目。

[#_14_12_容器管理控制]
== 14.12 容器管理控制

如第 2 章所述，容器通过 CMS 来管理。我建议暴露 CMS 的某些能力，以便团队能够充分利用自己的DevOps 能力，比如：

* 为他们的微服务设置环境变量；
* 指定在哪个集群（例如，测试、集成、生产）上运行微服务；
* 根据微服务的需要管理 CPU、内存和磁盘资源；
* 手动或根据 SLA 及处理滞后度增加或减少服务数量；
* 根据 CPU、内存、磁盘或滞后度量自动伸缩。

业务需要确定应该向开发人员暴露多少容器管理选项，以及应该由专门的操作团队管理多少选项。这通常依赖于组织内部的 DevOps 文化。

[#_14_13_集群创建和管理]
== 14.13 集群创建和管理

集群的创建和管理往往随着公司围绕事件驱动型微服务进行扩张而出现。一般来说，中小型公司通常可以通过使用单个事件代理集群来满足其所有服务需求。然而，由于各种技术和法律原因，大公司经常面临提供多个集群的压力。国际化公司可能需要在本国保存某些数据。尽管现代事件代理具有出色的横向伸缩能力，但数据量可能会增长得非常大，以至于实际上无法将所有数据都保存在一个集群中。组织中的各个业务单元可能需要自己的集群来实现隔离。也许最常见的情况是，必须跨多个区域中的多个集群复制数据，以便在集群完全中断的情况下提供冗余。

多集群管理，包括动态跨区域通信和灾难恢复，是一个非常复杂的主题，其内容完全可以填满专门讨论这一主题的图书。它还高度依赖于相关的服务以及所使用的预防和恢复策略。一些企业（如 Capital One）拥有大量的定制库和围绕其 Apache Kafka 实现构建的代码，以允许本地多集群复制。作为一家银行，Capital One 不能接受任何金融交易事件的丢失。你的需求可能有所不同。由于这些原因，本书不涉及多集群服务和数据管理策略。

[#_14_13_1_事件代理的程序化创建]
=== 14.13.1 事件代理的程序化创建

负责管理事件代理集群的团队通常还提供了创建和管理新集群的工具。也就是说，商业云提供商也正在进入这个领域。例如，Apache Kafka 集群现在可以在 AWS 中按需创建（截至 2018 年 11 月），其他一些云服务提供商也在加入。不同的事件代理技术可能需要不同的工作量来支持，领域专家应该对此进行仔细研究。无论哪种情况，目标都是拥有一个事件代理集群管理工具，整个组织都可以使用这个工具轻松地创建和伸缩事件代理。

[#_14_13_2_计算资源的程序化创建]
=== 14.13.2 计算资源的程序化创建

你通常需要创建一组独立于所有其他资源的计算资源。并不总是需要创建全新的容器管理服务，因为现有的通常可以服务于多个命名空间。与事件代理一样，云计算提供商通常会提供托管服务（比如谷歌和亚马逊的托管 Kubernetes 解决方案），可以按需为你提供这些功能。

适用于事件代理的技术和法律要求同样适用于计算资源。通过跨数据中心分布处理来避免区域故障，如果数据无法离开本国，则在本地处理数据，并通过动态地将计算繁重的工作负载转移到更便宜的服务提供商来节省资金。

通常你可以使用相同的持续集成和持续交付工具来执行这一任务，但是需要一个选择机制来确定在哪里部署微服务。此外，你还需要确保所需的事件数据可供计算资源使用，通常是通过同一区域或可用区域内的协同定位。跨区域通信一直是可行的，但其成本比较高且比较慢。

[#_14_13_3_跨集群事件数据复制]
=== 14.13.3 跨集群事件数据复制

在集群之间复制事件数据对于将事件驱动型微服务扩展到单个集群之外非常重要，示例场景包括灾难恢复、常规跨集群通信和以编程方式生成的测试环境。

如何在集群之间复制数据的细节因事件代理和复制工具实现而异。当选择一个复制工具实现时，要考虑以下几点。

* 是否能自动复制新添加的事件流？
* 如何处理删除或修改的事件流的复制？
* 数据是精确复制（有相同的偏移量、分区和时间戳）还是近似复制？
* 复制延迟是多少？业务需求是否可接受？
* 性能指标是什么？它能根据业务需要伸缩吗？

[#_14_13_4_工具的程序化创建]
=== 14.13.4 工具的程序化创建

最后，到目前为止讨论的所有工具集也应该以程序化方式提供给新集群。这提供了一组通用工具，你可以将其部署到任何集群，而无须依赖事件代理本身以外的任何数据存储。以这种方式使用工具有很多好处。首先，工具的使用频率要高得多，有助于揭示缺陷或需要添加的必要特性。其次，它降低了使用新集群的门槛，因为用户已经熟悉了工具界面。最后，当集群终止时，这些工具可以与集群一起终止，而不需要单独清理。

[#_14_14_依赖跟踪和拓扑可视化]
== 14.14 依赖跟踪和拓扑可视化

跟踪微服务之间的数据依赖关系对于组织运行事件驱动型微服务非常有用。唯一的要求是组织必须知道哪些微服务正在读写哪些事件流。为了实现这一点，可以采用一个自上报系统，让消费者和生产者上报自己的消费模式和生产模式。然而，自上报解决方案的问题在于，它实际上是自愿的，而且总会有一些团队忘记、退出，或者就是单纯地不愿意上报。在不是所有人都参与的情况下并不能很有效地确定依赖性，因为沟通结构中的漏洞和不完整的拓扑会限制洞察力。这就是本章前面讨论的权限结构和 ACL 发挥作用的地方。

利用权限结构来确定依赖关系确保了两件事情。首先，微服务在没有注册其权限需求的情况下无法运行，因为它将无法读取或写入任何事件流。其次，如果对权限结构进行了任何变更，则用于确定依赖关系和拓扑生成的相关权限也会更新。无须其他更改即可确保正确的依赖跟踪。

以下是此类工具的一些其他用途。

确定数据沿袭::

数据科学家和数据工程师经常遇到的一个问题是，如何确定数据从何而来以及是如何路由的。通过权限结构的完整图形，他们可以识别任何给定事件的每个祖先服务和流。这可以帮助他们追溯有缺陷的源代码，并确定给定数据转换中涉及的所有服务。请记住，可以在权限事件流和微服务–团队分配事件流中返回到某个时间点，以在该时间点生成拓扑视图。在审核旧数据时，这通常非常有用。

覆盖团队边界::

拥有微服务和流的团队可以映射到拓扑上。当使用适当的可视化工具呈现时，拓扑将清楚地显示哪些团队直接负责哪些服务。

发现数据源::

可视化程序对数据发现来说是一个很有用的工具。潜在消费者可以看到哪些流是可用的，以及它们的生产者和消费者是谁。如果需要更多关于流数据的信息，潜在消费者可以联系生产者。

度量互连性和复杂性::

正如微服务最好符合高内聚和低耦合一样，团队也是如此。有了这个工具，团队就可以度量微服务之间有多少内部连接以及有多少跨边界连接。一般的经验法则是，外部连接越少越好，但连接数仅是一个非常基础的指标。然而，采用这个基础指标也可以在一定程度上揭示团队之间的依赖性。

映射业务需求到微服务::

将微服务与业务需求对齐，可以将实现映射到业务需求。在代码库自述文件或微服务元数据存储中，将每个微服务的业务需求与其代码一起显式说明是合理的。反过来，这可以映射到所属的团队。

业务负责人可以查看这个覆盖图并扪心自问：“这个实现结构是否与团队的目标和优先级相一致？”这是企业可以使用的最重要的工具之一，以确保其技术团队与业务沟通结构保持一致。

[#_拓扑示例]
=== 拓扑示例

图 14-2 展示了一个包含 25 个微服务的拓扑结构，覆盖了 4 个团队的所有权。为清楚起见，每个箭头表示向事件流生成数据以及消费进程的消费。举例来说，微服务 3 正在消费来自微服务 4 的数据流。

.图 14-2：服务连接的拓扑映射
image::image495.png[]

该映射显示，团队 2 在负责两个不属于其主界限上下文（右下角）的微服务。如果团队 2 的业务目标与微服务 2 和微服务 7 所服务的功能不一致，则可能需要引起关注。此外，微服务 2 和微服务 7 有许多对团队 1、团队 3 和团队 4 的依赖，这增大了团队 2 暴露给外部的“表面积”。表 14-2 给出了互连性的度量。

.表 14-2：拓扑图的互连性度量
|===
| |输入流 |输出流 |输入团队连接 |输出团队连接 |拥有服务
|团队 1 |1 |3|1（团队 2）|2（团队 2、团队 3）|5
|团队 2 |8|2|3（团队 1、团队 3、团队 4） |2（团队 1、团队 4）|8
|团队 3 |3|3|2（团队 1、团队 4）|1（团队 2）|6
|团队 4 |1|5|1（团队 1）|2（团队 2、团队 3）|8
|===

下面来看看如果减少团队间的连接以及在团队边界上的输入和输出流数量会发生什么。微服务 2 和微服务 7 是很好的候选者，因为它们是拓扑中的所有权孤岛，可以重新分配以减少跨团队依赖的数量。微服务 7 可以被分配给团队 1（或团队 3），微服务 2 可以被分配给团队 4。现在更明显的是，微服务 1也可以被分配给团队 4，以进一步减少跨界通信。结果如图 14-3 和表 14-3 所示。

.图 14-3：重新分配微服务之后的服务连接的拓扑映射
image::image496.png[]

表 14-3：互连性的新度量，差异显示在括号中

|===
| |输入流 |输出流 |输入团队连接 |输出团队连接 |拥有服务
|团队 1 |1|3|1（团队3）|1（团队3）（–1）|5
|团队 2 |4（-4）|0（-2）|2（团队3、团队4）（–1）|0（–2）|6（–2）
|团队 3 |3|3|2（团队1、团队4）|2（团队1、团队2）（+1）|6
|团队 4 |1|3（–2）|1（团队1）|2（团队3、团队4）|8（+2）
|===

计算跨边界依赖关系有一个积极的结果：跨团队输入和输出流数量减少，团队之间净减少了 3 个连接。最小化跨边界连接的数量有助于优化团队的微服务分配。但是，这还不足以确定向团队分配微服务。你还必须考虑各种因素，比如团队的人数、专业领域和实现复杂性。

不过，最重要的是，你必须解释清楚微服务正在执行的业务功能。考虑一个场景，其中一个团队生成了大量的事件数据，这些数据也许来自许多外部源。可能该团队的业务职责仅限于获取数据并组织成事件，其业务逻辑由其他下游消费者执行。在这种情况下，团队将有许多流连接和团队系统间的连接。这正是有利于发现与团队负责的微服务相关联的业务功能的地方。

在前面的例子中，有许多问题值得探究。微服务 2 的业务功能实现是否更接近团队 2 或团队 4 的目标？微服务 7 呢？它是否比团队 2 更接近团队 1 的目标？一般来说，哪些服务最适合哪个团队？这些答案往往是定性的，必须根据团队的目标仔细评估。团队目标自然会随着业务需求的发展而变化，确保分配给这些团队的微服务与总体业务目标保持一致是很重要的。可视化工具提供了对这些分配的深入洞察，并有助于向业务所有者提供清晰的信息。

[#_14_15_小结]
== 14.15 小结

使用多个自主服务需要你仔细考虑如何管理这些系统。本章介绍的工具旨在帮助你的组织管理其服务。

随着服务数量的增加，任何一个人了解所有工作方式以及每个服务和流如何融入全局的能力都会减弱。保持对属性（比如服务和流所有权、schema 和元数据）的显式跟踪使得组织可以长期管理和跟踪变更。组织应优先考虑如何减少部落知识以及对简单流和服务属性的文档编撰。毕竟，围绕服务或流属性的任何描述的模糊性都会在新增的服务实例中被放大。

赋予团队自主性和对服务的控制是大规模管理微服务的重要方面。根据 DevOps 原则，你应该能够重置消费者组偏移量和微服务状态存储。

schema 有助于对事件的意义达成共识。schema 注册表提供了一种高效的 schema 管理机制，可以用来通知相关方对特定 schema 的任何变更。
