= 第 11 章 使用重量级框架的微服务

本章和第 12 章将介绍事件驱动处理中最常用的全功能框架。这些框架通常被称为流框架，它们提供了处理数据流的机制和 API，并且经常用于向事件代理消费和生成事件。这些框架大致可以分为本章介绍的重量级框架和第 12 章介绍的轻量级框架。这两章的目的不是比较这些技术，而是对这些框架如何工作进行概述。然而，有些内容会研究特定框架的特性，特别是当它们涉及以类似微服务的方式实现应用程序时。为了介绍重量级框架，本章会用 Apache Spark、Apache Flink、Apache Storm、Apache Heron 和Apache Bean 模型的概念作为例子。

重量级流框架的其中一个定义属性是它需要独立的处理资源集群来执行操作。这个集群通常由许多可共享的工作者节点以及一些负责安排和协调工作的主节点组成。此外，领先的 Apache 解决方案传统上依赖于Apache ZooKeeper（另一个集群服务）来提供高可用性支持和协调集群领导者的选举。尽管 ZooKeeper对于将重量级集群引入生产环境并不是绝对必要的，但是在创建自己的集群时，你应该仔细评估是否需要它。

重量级框架的第二个定义属性是它有自己的失败处理、恢复、资源分配、任务分配、数据存储、通信以及处理实例和任务之间进行协调的内部机制。与轻量级框架、FaaS 和 BPC 实现不同，这些方案重度依赖于CMS 和事件代理来实现上述功能。

这两个属性就是这些框架为什么被称为“重量级”的主要原因。必须独立于事件代理和 CMS 来管理和维护额外的集群框架，这不是一个小任务。

image:image3.png[image,width=40,height=46]
一些重量级框架正在向类似轻量级的执行模型演进。这些轻量级模型能与用于操作其他微服务实现的 CMS 很好地集成。

你可能已经注意到，重量级框架做了很多 CMS 和事件代理已经处理的事情。CMS 可以管理资源分配、失败、恢复和系统伸缩，而事件代理可以提供微服务实例之间基于事件的通信。重量级框架是融合了 CMS和事件代理能力的独立解决方案。第 12 章将展开更多关于轻量级框架的话题。

[#_11_1_重量级框架的简单历史]
== 11.1 重量级框架的简单历史

重量级流处理框架直接继承自它们的重量级批处理前辈，其中最广为人知的是发布于 2006 年的 Apache Hadoop，它提供了所有人都可以使用的开源大数据技术。Hadoop 将许多技术绑定在一起以提供海量并行处理、失败恢复、数据持久性和内部节点通信等功能，它使得用户可以廉价地使用硬件并轻松地解决需要数千个（或更多）节点才能解决的问题。

MapReduce 是最早广泛使用的处理超大批量数据（又称大数据）的方法之一，不过，虽然功能强大，但与当今的许多方案相比，它的执行速度很慢。随着时间的推移，大数据的规模在稳步增长。尽管早期数百（或数千）GB 的工作负载很常见，但今天的工作负载已经扩展到了 TB 和 PB 的规模。随着这些数据集的增长，对更快的处理、更强大的方案、更简单的执行方案以及能够提供近实时流处理能力的解决方案的需求也随之增长。

这就是 Spark、Flink、Storm、Heron 和 Beam 发挥作用的地方。开发这些解决方案是为了处理数据流，并比基于批处理的 MapReduce 作业更快地提供可操作的结果。一些解决方案，如 Storm 和 Heron，是单纯处理数据流的技术，它们当前还没有提供批处理能力。其他一些解决方案，如 Spark 和 Flink，则将批处理和流处理融合到了一个解决方案中。

这些技术无疑是大多数大数据爱好者所熟悉的，而且可能已经在许多组织的数据科学和分析部门得到了一定程度的应用。事实上，当这些团队将现有的基于批处理的作业转换为基于流的管道时，就是许多组织开始涉足事件驱动处理的原因。

[#_11_2_重量级框架的内部运作]
== 11.2 重量级框架的内部运作

前面提到的重量级开源 Apache 框架都是以非常类似的方式运行的。像谷歌的 Dataflow 这样的专有解决方案，是使用 Apache Beam 的 API 编写的应用程序来执行的，可能是以类似的方式运行，但这只是一个假设，因为它并没有开放源代码，也没有对后端机制的详细描述。详细描述重量级框架的一个挑战是，每个框架都有自己的操作和设计细节，而对每个框架的全面介绍远远超出了本章所能涵盖的范围。

image:image3.png[image,width=40,height=46]
对于你所选择的重量级框架，一定要仔细阅读介绍其运行机制的文档。

重量级流处理集群是一组专用的处理和存储资源，分为两个主要角色。第一个角色是主节点，它对工作者节点上的执行者和任务进行优先级排序、分配和管理。第二个角色是执行者，它使用工作者节点可用的处理能力、内存、本地和远程磁盘来完成这些任务。在事件驱动处理中，这些任务会连接到事件代理并从事件流中消费事件。图 11-1 展示了这个工作的粗略分解。

.图 11-1：重量级流处理框架通用视图
image::image195.png[]

图 11-1 也展示了 Apache ZooKeeper，它扮演着这个流集群的支持角色。Zookeeper 提供了高可靠的分布式协调能力，并用于确定负责的主节点（因为节点发生故障的情况并不少见，无论是工作者节点、主节点还是 ZooKeeper 节点）。当主节点发生故障时，ZooKeeper 会帮助确定剩余的主节点中哪一个是新的领导者，以确保操作的连续性。

image:image4.png[image,width=40,height=46]
ZooKeeper 在历史上一直是提供分布式重量级框架协调能力的主要组件。新的框架可能使用也可能不使用 ZooKeeper。无论哪种情况，分布式协调对于可靠运行分布式负载都是至关重要的。

作业是用框架的软件开发包（SDK）构建的流处理拓扑，其作用是解决具体界限上下文的问题。它在集群中持续运行，在事件到达时处理事件，就像本书中描述的任何其他微服务一样。

作业被提交到集群之后，其所定义的流处理拓扑被分解为任务并分配给工作者节点。任务管理器会监控任务并确保它们得到完成。当发生故障时，任务管理器会挑选一个可用的执行者重新开启工作。任务管理器通常是按高可用标准搭建的，这样如果任务管理器运行的节点发生故障，则可以由备份节点替代，以防止所有正在运行的作业发生失败。

图 11-2 展示了一个通过主节点 1 提交到集群的作业，而该作业又被转换成了任务供执行者处理。这些长期运行的任务建立了到事件代理的连接，并开始从事件流消费事件。

.图 11-2：提交流处理作业以从事件流中读取数据
image::image196.png[image,width=342,height=236]

虽然这个例子展示了任务和流分区 1∶1 映射的情况，但是我们也可以配置希望应用程序使用的并行数量。既可以让一个任务消费所有分区，也可以让多个任务消费同一个分区，比如使用队列的情况。

[#_11_3_优点和局限性]
== 11.3 优点和局限性

本章讨论的重量级框架主要用于分析技术。它们对于用近乎实时的方式分析大量事件以快速做出决定有重大的价值。以下是一些相当常见的使用模式：

* 抽取数据、转换并将其加载到新的数据存储（ETL）中；
* 执行基于会话和窗口的分析；
* 发现异常行为模式；
* 聚合流并维持状态；
* 执行任何类型的无状态流操作。

这些框架功能强大且相当成熟，许多组织在使用它们，并对其源代码做出了贡献。有大量的图书和博客文章、出色的文档以及许多示例应用程序可供你查阅。

然而，这些框架有几个相当重要的缺点会对基于它们的微服务应用程序产生限制（但并不是说完全不可用）。

首先，这些重量级框架最初的设计并没有考虑微服务类型的部署。部署这些应用程序需要一个专用的资源集群，而不仅仅是事件代理和 CMS，这增加了管理大规模应用程序的复杂性。随着新的部署技术的开发，有一些方法可以减轻这种复杂性，其中一些将在本章后面详细介绍。

其次，这些框架大部分是基于 Java 虚拟机（Java virtual machine，JVM）的，这限制了用来创建单独的微服务应用程序的实现语言。常见的解决方法是使用重量级框架作为独立的应用程序来执行转换操作，而采用另一种语言的独立应用程序通过转换后的状态存储提供业务功能。

再次，并非所有框架都支持将实体流物化为永久保留的表。这就排除了创建表联结和流–表联结以及图10-3 所示的浇注模式之类的实现模式。

即使重量级框架确实支持流的物化和联结，在文档中也常常不会很明显地体现这一特性。这些框架中有很多框架会将重点聚焦于基于时间的聚合，例如，相关的博客文章和广告都在强调时间序列分析和基于有限窗口大小的聚合。仔细挖掘后会发现，主流框架提供了一个全局窗口，该窗口允许事件流的物化。从这里开始，你可以实现自己的自定义联结特性，尽管如此，考虑到它们在组织中处理大规模事件流的重要性，这些特性的文档化和展示程度仍然远远不如它们应有的好。

以上缺点再一次表明了这些框架在设计和实现时，主要考虑的是应对分析类型的工作负载。对每个实现的技术改进和对独立于实现的公共 API（如 Apache Beam）的优化投入正在推动重量级框架领域的不断变化，而这些框架的新版本能带来什么很值得我们关注。

[#_11_4_集群搭建方案和执行模式]
== 11.4 集群搭建方案和执行模式

搭建和管理重量级流处理集群有许多可选的方案，每一个选项都有其优点和缺点。

[#_11_4_1_使用托管服务]
=== 11.4.1 使用托管服务

首先，管理集群最简单的方式就是付钱让别人来做。正如有许多计算服务供应商一样，也有一些供应商愿意为你托管集群并管理你的大部分操作需求。与启动自己的集群的预算成本相比，这通常是最昂贵的方案，但它大大减少了操作开销，并消除了对内部专业知识的要求。例如，亚马逊提供了托管 Flink 和Spark 服务；谷歌、Databricks 和微软提供了它们自己的 Spark 捆绑服务；谷歌提供了 Dataflow（它自己实现的 Apache Beam 的运行器）。

需要注意的一点是，这些服务似乎在不断向完全无服务器的方式发展，在这种方式中，整个物理集群对作为订阅者的你是不可见的。根据你的安全性、性能和数据隔离要求的不同，这种方式可能是可接受的，也可能是不可接受的。请确保了解这些服务供应商能提供什么和不能提供什么，因为它们可能不包括独立运行的集群的所有功能。

[#_11_4_2_构建自己的完整集群]
=== 11.4.2 构建自己的完整集群

一个重量级的框架可能有自己独立于 CMS 的专用可伸缩资源集群。这种部署是重量级集群的历史常态，因为它密切模拟了原始的 Hadoop。当重量级框架被需要大量（成百上千个）工作者节点的服务使用时，这是很常见的。

[#_11_4_3_使用cms集成来创建集群]
=== 11.4.3 使用CMS集成来创建集群

集群也可以连同 CMS 一起被创建。第一种模式只涉及在 CMS 提供的资源上部署集群，而第二种模式涉及利用 CMS 本身作为伸缩和部署单个作业的手段。用 CMS 部署集群的主要优点是可以获得 CMS 提供的监控、日志和资源管理功能。伸缩集群也变成了简单地添加或移除所需节点类型的操作。

pass:[1. 使用 CMS 部署和运行集群]

使用 CMS 部署重量级集群有许多优点。主节点、工作者节点和 ZooKeeper（如果适用的话）在它们自己的容器或虚拟机中启动。这些容器像其他容器一样受到管理和监控，CMS 提供了故障可视化以及自动重启这些实例的方法。

image:image3.png[image,width=40,height=46]
你可以强制静态分配主节点和其他任何需要高可用性的服务，以防止 CMS 在伸缩底层计算资源时将它们搞乱。这可以防止来自集群监控器的关于主节点丢失的过度警报。

pass:[2. 使用 CMS 为单个作业指定资源]

历史上，重量级集群要负责为每个提交的应用程序分配和管理资源。近年来 CMS 的引入可以为你做同样的事情，同时它也可以管理所有其他的微服务实现。当重量级集群需要更多的资源来扩容时，它必须首先从 CMS 请求并获取资源。可以将这些资源添加到集群的资源池中，最后根据需要分配给应用程序。

Spark 和 Flink 使你能够直接利用 Kubernetes 进行可伸缩的应用程序部署，而不仅仅是其原来的专用集群配置，其中每个应用程序都有自己的专用工作者节点集。例如，Apache Flink 允许应用程序使用 Kubernetes 在它们自己的独立会话集群中独立运行。Apache Spark 提供了类似的选项，允许 Kubernetes 扮演主节点的角色，并为每个应用程序维护隔离的工作资源。图 11-3 是这一工作原理的基本概述。

.图 11-3：Kubernetes 集群部署和管理的一个作业
image::image220.png[]

image:image3.png[image,width=40,height=46]
此部署模式与部署非重量级微服务的方式几乎相同，并将轻量级和 BPC 部署策略融合在了一起。

这种部署模式有以下优点：

* 利用了 CMS 的资源获取模型，包括伸缩需求；
* 作业之间是完全隔离的；
* 可以使用不同的框架和版本；
* 重量级流应用程序可以像微服务一样处理，并且使用相同的部署流程。

当然，这种模式也有一些缺点：

* 不是所有的主流重量级流框架都支持此模式；
* 不是所有主流的 CMS 都支持这种集成；
* 可能还无法支持如自动伸缩这种在完全的集群模式中可用的特性。

[#_11_5_应用程序提交模式]
== 11.5 应用程序提交模式

将应用程序提交到重量级集群进行处理有两种主要方式：驱动器模式和集群模式。

[#_11_5_1_驱动器模式]
=== 11.5.1 驱动器模式

Spark 和 Flink 支持驱动器模式。尽管驱动器在集群资源内运行，但它仅仅是一个帮助协调和执行用户应用程序的独立的本地程序。驱动器协调集群以确保应用程序的进度，并可用于报告错误、执行日志记录和完成其他操作。值得注意的是，驱动器的终止会导致应用程序的终止，这提供了一种部署和终止重量级流应用程序的简单机制。可以使用 CMS 把应用程序驱动器当作微服务进行部署，并从重量级集群中获取工作者资源。终止驱动器的操作和停止任何其他微服务一样。

[#_11_5_2_集群模式]
=== 11.5.2 集群模式

Spark 和 Flink 支持集群模式，并且这也是 Storm 和 Heron 作业的默认部署模式。在集群模式下，整个应用程序被提交到集群进行管理和执行，然后将一个唯一的 ID 返回给调用函数。这个唯一的 ID 对于识别应用程序和通过集群的 API 向其发出命令是必需的。在这种部署模式下，命令必须直接与集群通信进而部署和停止应用程序，这可能不适合你的微服务部署管道。

[#_11_6_处理状态和使用检查点]
== 11.6 处理状态和使用检查点

可以使用内部状态存储或外部状态存储（参见第 7 章）来持久化有状态的操作，尽管为了高性能和高可伸缩性，大部分重量级框架倾向于内部状态存储。有状态的记录保存在内存中以供快速访问，但是当状态增长到超出可用内存时，出于数据持久化目的，状态也会溢出到磁盘上。使用内部状态存储确实会带来一些风险，比如磁盘故障导致的状态丢失、节点故障以及 CMS 主动伸缩导致的临时状态中断。然而，能获得的性能提升往往远远超过潜在的风险，而风险可以通过仔细的规划来缓解。

检查点是应用程序当前内部状态的快照，可在伸缩或节点故障发生之后用于重建状态。为了防止数据丢失，可以将检查点持久化到应用程序工作者节点外部的持久性存储器上。保存检查点可以使用所有与框架兼容的存储方案，比如 Hadoop 分布式文件系统（HDFS，一种通用选项）或者高可用的外部数据存储。然后，每个分区状态存储可以从检查点进行恢复，这在应用程序发生故障的情况下提供了完全恢复功能，在伸缩和工作者节点发生故障的情况下提供了部分恢复功能。

当消费和处理分区的事件流时，检查点机制必须考虑两个主要状态。

算子状态::

指 <partitionId,offset> 这对值。检查点必须确保内部键的状态（参见下一条）能匹配上每个分区的消费者偏移量。每个 partitionId 在所有输入主题中是唯一的。

键控状态::

指 <key,state> 这对值。这是与键控实体相关的状态，比如在聚合、缩减、窗口化、联结和其他有状态操作期间的状态。

算子状态和键控状态必须同步记录，以便键控状态能够准确地表示算子状态已消费的所有事件的处理。如果无法同步记录，则可能导致事件无法被全部处理或者会被重复处理。记录到检查点的状态例子如图 11-4 所示。

.图 11-4：有算子状态和键控状态的检查点
image::image322.png[]

image:image3.png[image,width=40,height=46]
恢复检查点状态在功能上等同于使用快照恢复外部状态存储，如 7.4.3 节所述。

在从检查点完全加载与应用程序任务相关的状态之后，你才能处理新的数据。重量级框架还必须验证每个任务的算子状态与相关的键控状态是否匹配，以确保在任务之间正确分配分区。本章开头介绍的每个主流的重量级框架都有自己的检查点实现方式，因此一定要查看相应的文档以了解详细信息。

[#_11_7_伸缩应用程序和处理事件流分区]
== 11.7 伸缩应用程序和处理事件流分区

重量级应用程序的最大并行度受第 5 章中讨论的因素的限制。一个典型的有状态流处理器会受限于最小分区流的分区数。因为重量级的处理框架特别适合计算大量用户生成的数据，所以在白天看到具有大量计算需求夜晚却很少的循环模式是很常见的。一个每日循环模式的例子如图 11-5 所示。

.图 11-5：数据量每日循环的例子
image::image323.png[]

处理此类数据的应用程序会极大地受益于能随着需求的增加而扩容以及随着需求的减少而缩容的能力。适当的伸缩可以确保应用程序有足够的容量及时处理所有事件，不会因过度配置而浪费资源。理想情况下，接收事件到完全处理完事件之间的延迟应该尽可能小，尽管许多应用程序对暂时升高的延迟并不敏感。

image:image5.png[image,width=47,height=44]
伸缩应用程序与伸缩集群是不同的。这里讨论的所有伸缩都是在集群有足够资源的前提下来提升应用程序的并行度。有关集群资源的伸缩，请参阅框架文档。

无状态流的应用程序非常易于扩缩容。应用程序的新处理资源可以被轻易地加入消费者组或从中移除，加入或移除时，消费者组会进行资源再平衡并恢复流处理。处理有状态的应用程序则要困难得多：不仅要加载状态到分配给应用程序的工作者节点上，还要让加载的状态匹配输入事件流分区的分配。

伸缩有状态的应用程序有两种主要策略，虽然具体的策略因技术而异，但它们有一个共同的目标，即最小化应用程序的中断时间。

[#_11_7_1_伸缩运行中的应用程序]
=== 11.7.1 伸缩运行中的应用程序

第一个策略允许你移除、添加或重新分配应用程序实例，而不会停止应用程序或影响处理的准确性。只有某些重量级流框架具有这种能力，因为它需要仔细地处理状态和洗牌事件。实例的添加和移除都需要重新分布所有已分配的流分区并从上一个检查点重新加载状态。图 11-6 展示了一个常规的洗牌，其中每个下游的 reduce 操作都从来自上游的 groupByKey 操作产生的洗牌事件中获取数据。如果其中一个groupByKey 实例突然终止了，那么 reduce 节点将不再知道从哪里获得洗牌事件，这会导致致命异常。

.图 11-6：常规洗牌的逻辑表现
image::image324.png[]

Spark 的动态资源分配实现了这种伸缩策略。但是，它需要使用粗粒度模式进行集群部署，并使用外部洗牌服务（external shuffle service，ESS）作为隔离层。如图 11-7 所示，ESS 会接收来自上游任务的

洗牌事件，并存储这些事件以供下游任务消费。下游消费者通过向 ESS 问询分配给它们的数据来访问事件。

.图 11-7：使用 ESS 的逻辑表现
image::image325.png[]

现在可以终止任务的执行者 / 实例了，因为下游操作不再依赖于特定的上游实例。如图 11-8 所示，洗牌后的数据保留在 ESS 中，缩容之后的服务可以重新进行处理。在本例中，实例 0 是唯一剩余的处理器，承担着两个分区的任务，而下游操作通过与 ESS 的接口无缝地继续处理。

.图 11-8：使用 ESS 的已缩容的应用程序（注意实例 1 已经不在了）
image::image326.png[]

image:image3.png[image,width=40,height=46]
实时事件流中的洗牌仍然是重量级框架正在开发的一个领域。第 12 章将研究轻量级框架如何直接利用事件代理来扮演 ESS 的角色。

谷歌的 Dataflow（所执行的应用程序是用 Beam API 编写的）提供了资源和工作者实例的内置伸缩。Heron 提供了一个（目前处于实验阶段的）健康管理器，它可以使拓扑结构具有动态性和自调节性。此功能仍在开发中，但其旨在实现拓扑的实时、有状态的伸缩。

==== 重量级框架的持续改进

在本书英文版出版前不久，Apache Spark 3.0.0 版本发布了。这个版本的一个主要变动是，无须使用 ESS 便可动态调整实例数量。

这种模式的工作原理是跟踪生成洗牌文件的阶段，并且当下游作业仍处于活动状态时，就要使生成该数据的执行者保持活动状态。实际上，这个数据源扮演着自己的 ESS 的角色。一旦所有下游作业不再需要它们的洗牌文件，它们还允许进行自我清理。

为 ESS 提供专用和持久的独立存储使得 CMS 无法完全扩展作业，因为必须始终投入足够的资源来确保 ESS 可用。Spark 中的这个新的动态伸缩选项说明了重量级框架与 CMS（如Kubernetes）的进一步集成，事实上，它是 JIRA ticket 描述的新特性中提到的主要用例之一。

[#_11_7_2_通过重启伸缩应用程序]
=== 11.7.2 通过重启伸缩应用程序

第二个策略是通过重启对应用程序进行伸缩，所有的重量级流框架都支持该策略。流的消费会暂停，应用程序会创建检查点，然后停止程序。接下来，使用新的资源和并行度重新初始化应用程序，并根据需要从检查点重新加载有状态数据。例如，Flink 提供了一个简单的 REST 机制来实现这个功能，而 Storm 提供了自己的再平衡命令。

[#_11_7_3_自动伸缩应用程序]
=== 11.7.3 自动伸缩应用程序

自动伸缩是根据特定指标自动伸缩应用程序的过程。这些指标可能包括处理延迟、消费者滞后度、内存使用率和 CPU 使用率等。一些框架有自建的自动伸缩选项，比如谷歌的 Dataflow 引擎、Heron 的健康管理器和 Spark Streaming 的动态分配功能。其他框架可能会要求你收集自己的性能和资源利用率指标，并将它们关联到框架的伸缩机制中，比如 14.10 节讨论的滞后度监控工具。

[#_11_8_从故障中恢复]
== 11.8 从故障中恢复

重量级集群被设计为对长期运行的作业所不可避免的故障具有高度的容忍度。主节点、工作者节点和ZooKeeper 节点（如果适用的话）的故障都可得到减轻，以允许应用程序在几乎不中断的情况下继续运行。这些容错功能内置于集群框架中，但在部署集群时可能需要额外的配置。

如果工作者节点发生故障，那么在该节点上正在运行的任务就会被转移到其他可用的节点上。任何所需的内部状态都会从最近的检查点与分区分配一起重新加载。主节点故障对于已经执行的应用程序应该是透明的，但是根据集群的配置，在主节点中断期间，你可能无法部署新作业。ZooKeeper（或类似技术）支持的高可用模式可以减少主节点丢失的影响。

image:image3.png[image,width=40,height=46]
请确保主节点和工作者节点有合适的监控和告警。虽然单个集群节点故障不一定会停止任务处理，但它仍然会降低性能并妨碍应用程序从连续故障中恢复。

[#_11_9_考虑多租户问题]
== 11.9 考虑多租户问题

随着集群中应用程序数量的增长，除了要考虑集群管理的开销问题，还必须考虑多租户问题。具体来说，应该考虑资源获取的优先级、备用资源与已提交资源的比率，以及应用程序可以声明资源的（伸缩）速率。例如，一个新的从其输入主题的最开始时间开启消费的流应用程序可能会请求并获取大部分的空闲集群资源，从而限制所有当前运行的应用程序获取它们自己的资源。这会导致应用程序达不到其服务级别目标（service-level objective，SLO），并产生下游业务问题。

以下是缓解这些挑战的两种方法。

运行多个小型集群::

每个团队或业务单元可以拥有自己的集群，它们跟其他集群完全隔离。最好的方法是，你可以通过编程方式来请求集群资源以保持较低的操作开销（无论是自己开发还是使用第三方供应商）。由于在协调节点（例如，主节点和 Zookeeper 节点）和监控/管理集群方面的开销，这种方法可能会带来较高的财务成本。

命名空间::

一个集群可以划分出多个具有专用资源的命名空间。每个团队或业务小组可以在自己的命名空间内分配自己的资源。在该命名空间内执行的应用程序只能获取这些资源，从而防止它们主动获取其他命名空间的资源而使其他应用程序陷入饥饿状态。此选项的一个缺点是，即使不需要，也必须将备用资源分配给每个命名空间，这可能会导致更大的空闲资源碎片池。

[#_11_10_语言和语法]
== 11.10 语言和语法

重量级流处理框架植根于其前身的 JVM 语言中，Java 是最常用的，其次是 Scala。Python 也是一种常见的语言，因为它在数据科学家和机器学习专家中很流行，这两类人在这些框架的传统用户中占了很大一部分。MapReduce 风格的 API 是很常用的，其中的操作会链接在一起作为数据集上的固定操作。重量级框架的 API 支持的语言相当有限。

类 SQL 的语言也越来越普遍。这使得可以用 SQL 转换来表示拓扑，并且减少了学习新框架的特定 API的认知成本。Spark、Flink、Storm 和 Beam 都提供了类 SQL 语言，尽管它们在功能和语法上有所不同，而且并非所有操作都受支持。

[#_11_11_选择一个框架]
== 11.11 选择一个框架

选择重量级流处理框架与选择 CMS 和事件代理非常类似。你必须确定你的组织愿意批准多少运营开销，以及其是否足以支持大规模运行完整的生产集群。这种开销包括常规的操作任务，比如监控、伸缩、故障排除、调试和分配成本，所有这些都是实现和部署实际应用程序所必需的。

软件服务供应商可能会将这些平台作为一种服务来提供，尽管选项往往比为 CMS 和事件代理选择供应商更为有限。你要评估可供选择的选项并相应地进行选择。

框架的流行度将影响你的决策。Spark 是最流行的框架，Flink 和 Storm 虽没那么流行但仍然在被很多人积极使用。通过 Apache Beam，可以独立于重量级框架的运行时来编写应用程序，尽管这可能对你的组织没有用处。Heron 是 Storm 的一种改进版本，提供了更高级的功能，似乎最不流行。将你在选择 CMS和事件代理时考虑的因素同样应用于选择或放弃重量级框架中来。

image:image5.png[image,width=47,height=44]
请记住，一个重量级的流框架并不能合理地实现所有事件驱动型微服务。在决策之前，请确认它是问题空间的正确解决方案。

[#_11_12_示例点击和观看的会话窗口]
== 11.12 示例：点击和观看的会话窗口

想象一下，你现在正在运作一家简易的在线广告公司。你在互联网上购买广告空间并转卖给客户。这些客户想要看到自己的投资回报，这可以用看到广告的用户的点击率来衡量。此外，客户可以按会话计费，其中会话被定义为连续的用户活动，空闲时间不超过 30 分钟。

在这个例子中有两个事件流：用户观看广告和用户点击广告。其目标是将这两个流聚合到会话窗口中，并在用户不执行任何新操作的 30 分钟的事件时间（不是挂钟时间）过去后发出会话事件。可以参阅第 6章，对流时间和水位的知识进行复习。

通常在收集这些行为事件时，你会期望在值字段中看到其他信息，比如广告发布的位置、用户的 Web 浏览器或设备信息，或者其他各种上下文或元数据。在本例中，观看和点击事件流都简化为表 11-1 所示的基本 schema 格式。

表 11-1：基本schema格式的观看和点击事件流

|===
|键 |值 |时间戳
|String userId
|Long advertisementId
|Long createdEventTime（创建事件的本地时间）
|===

你需要执行以下操作。

. 将相同的键都编在一组，以保证同一个用户的所有事件都是处理实例的本地事件。
. 使用具有 30 分钟超时的窗口将事件聚合在一起。
. 一旦 30 分钟时限到达，就发出包含一系列事件的窗口事件。

输出流遵照表 11-2 所示的格式。

表 11-2：带有窗口的开始时间和结束时间的输出流

|===
|键 |值
|<Window windowId,String userId>
|Action[] sequentialUserActions
|===

这个窗口对象表明了窗口的开始时间和结束时间。这是组合键的一部分，因为随着时间的推移，用户将有多个会话窗口，并且会话窗口可能会在用户之间重复。这个组合键确保了唯一性。值字段中的 Action 对象数组用于按顺序存储操作，并使得微服务可以计算哪些广告的观看导致了可付费的用户点击。Action类如下所示。

----
Action {
    Long eventTime;
    Long advertisementId;
    Enum action; //枚举项为Click和View
}
----

以下这个简化的 Apache Flink 源代码使用 MapReduce 风格的 API 显示了该例的拓扑。

----
DataStream clickStream = ... //创建点击事件流
DataStream viewStream = ... //创建观看事件流

clickStream
    .union(viewStream)
    .keyBy(<key selector>)
    .window(EventTimeSessionWindows.withGap(Time.minutes(30)))
    .aggregate(<aggregator function>)
    .addSink(<producer to output stream>)
----

该拓扑的可视化表示如图 11-9 所示，它使用的并行度是 2（注意有两个单独的实例）。

.图 11-9：来自用户观看和点击的会话创建处理拓扑
image::image327.png[]

阶段 1::

给每个实例的执行者分配它们的任务，然后反过来将输入事件流的分区分配给它们进行处理。点击流和观看流会被联合到一个单一的逻辑流中，然后根据 userId 键进行分组。

阶段 2::

keyBy 算子，连同下游的 window 算子和 aggregate 算子，需要将现在合并的事件洗牌到正确的下游实例。给定键的所有事件都在同一实例中消费，这为接下来的操作提供了必要的数据局部性。

阶段 3::

现在可以为每个用户生成会话窗口了，每个用户的事件都是单个实例的本地事件。事件按时间戳顺序被添加到本地状态存储中，聚合函数被应用于每个事件，直到检测到 30 分钟或更长时间的中断。此时，事件存储将清除已完成的会话，并在内存中清除 <windowId,userId> 键和值。

image:image3.png[image,width=40,height=46]
你所使用的框架也许可以对窗口和基于时间的聚合施加额外的控制（包括保留已关闭一段时间的会话和窗口），以便处理迟到的事件并向输出流发送更新。请查看框架的文档以获得更多信息。

接下来，图 11-10 展示了减少到只有一个并行度的效果。假设没有动态伸缩，则需要先停止流处理器，然后通过设置新的并行度从检查点还原它。服务启动时会从最后一个良好的检查点读取有状态的键控数据，并为算子恢复其对应分区的状态。一旦状态恢复，服务就可以恢复正常的流处理。

.图 11-10：没有并行度的会话创建处理拓扑
image::image328.png[]

阶段 1 的操作与以前一样，不过在本例中，实例 0 中的任务要消费所有分区。分组和洗牌仍在执行，尽管源和目标是在相同的实例上（如阶段 2 中见到的）。请记住，在实例 0 上运行的各个任务都必须消费分配给它们的洗牌事件，尽管这里的所有通信都是完全本地的。在拓扑的最后一个阶段（阶段 3），对事件的窗口化和聚合操作与之前一样。

[#_11_13_小结]
== 11.13 小结

本章介绍了重量级的流处理框架，包括它们的发展简史以及它们在解决问题过程中存在的一些问题。这些系统具有高度可伸缩性，使你可以根据各种分析模式来处理流，但它们可能不足以满足某些有状态的事件驱动型微服务模式的要求。

重量级框架使用集中式资源集群进行操作，这可能需要额外的运营开销、监控和协调才能成功地集成到微服务框架中。与容器管理解决方案（如 Kubernetes）集成的创新部署模型，可以更细粒度地部署重量级流处理器，类似于部署完全独立的微服务。
